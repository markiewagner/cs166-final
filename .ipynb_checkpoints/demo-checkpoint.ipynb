{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import scipy.stats as stats\n",
    "import json \n",
    "import nbinteract as nbi\n",
    "from sklearn import svm\n",
    "from collections import Counter\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     1,
     52,
     55
    ]
   },
   "outputs": [],
   "source": [
    "class CountMinSketch:\n",
    "    def __init__(self, eps, delta):\n",
    "        self.eps = eps\n",
    "        self.delta = delta\n",
    "        self.w = math.ceil(np.exp(1) / eps)\n",
    "        self.d = math.ceil(np.log(1 / delta))\n",
    "        self.tables = np.zeros((self.d, self.w))\n",
    "        self.backup = {}\n",
    "\n",
    "    def compute_hash(self, value, table_no):\n",
    "        fn = hashlib.md5()\n",
    "        inp = str(value) + str(0) + str(table_no)\n",
    "        fn.update(inp.encode())\n",
    "        out = int(fn.hexdigest(), 16)\n",
    "        return out % self.w\n",
    "\n",
    "    def count(self, value):\n",
    "        if str(value) in self.backup: \n",
    "            self.backup[str(value)] = self.backup[str(value)] + 1\n",
    "        else:\n",
    "            self.backup[str(value)] = 1\n",
    "        for i in range(self.d):\n",
    "            j = self.compute_hash(value, i)\n",
    "            self.tables[i][j] = self.tables[i][j] + 1\n",
    "\n",
    "    def estimate(self, value):\n",
    "        ests = []\n",
    "        for i in range(self.d):\n",
    "            j = self.compute_hash(value, i)\n",
    "            ests.append(self.tables[i][j])\n",
    "        return min(ests)\n",
    "\n",
    "    def real_estimate(self, value):\n",
    "        if str(value) in self.backup: return self.backup[str(value)]\n",
    "        return -1\n",
    "\n",
    "    def compute_size(self):\n",
    "        size = 0\n",
    "        for key in self.backup:\n",
    "            size += abs(self.backup[key])\n",
    "        return size\n",
    "\n",
    "    def save_counts(self, count_filename='counts.txt', actual_filename='backups.txt'):\n",
    "        np.savetxt(count_filename, self.tables)\n",
    "        with open(actual_filename, 'w') as fp: json.dump(self.backup, fp)\n",
    "\n",
    "    def load_counts(self, count_filename='counts.txt', actual_filename='backups.txt'):\n",
    "        with open(actual_filename, 'r') as fp: \n",
    "            temp = json.load(fp)\n",
    "            self.backup = temp\n",
    "        self.tables = np.loadtxt(count_filename)\n",
    "        \n",
    "def load_data(cms, data):\n",
    "  for el in data:\n",
    "    cms.count(el)\n",
    "def generate_sample(n=1000, dist='uniform', loc=0, scale=100, lambda_=5, s=100):\n",
    "    if dist == 'uniform':\n",
    "        float_sample = stats.uniform.rvs(loc, scale, n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'zipf':\n",
    "        float_sample = stats.zipf.rvs(a, size=n)\n",
    "        return [(el) for el in float_sample]\n",
    "    if dist == 'exp':\n",
    "        float_sample = planck.rvs(lambda_, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'lognorm':\n",
    "        float_sample = lognorm.rvs(s=s, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'geometric':\n",
    "        float_sample =  geom.rvs(p, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    elif dist == 'normal':\n",
    "        float_sample = stats.norm.rvs(loc, scale, n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     3,
     17,
     35
    ]
   },
   "outputs": [],
   "source": [
    "# VISUALIZE ERROR PROBABILITY VS DELTA\n",
    "\n",
    "# compute empirical probability of error exceeding the threshold\n",
    "def compute_error_prob(cms, data, n):\n",
    "  err = []\n",
    "  for el in data:\n",
    "    err.append(cms.estimate(el) - cms.real_estimate(el))\n",
    "  avg_err = sum(err) / len(err)\n",
    "  max_err = max(err)\n",
    "  exceed = 0\n",
    "  for el in err:\n",
    "    if el > cms.eps * n:\n",
    "      exceed += 1\n",
    "  p = exceed / len(err)\n",
    "  return p, avg_err, max_err, err\n",
    "\n",
    "# run experiments on 10 values of delta interpolated between (min_delta, max_delta) and compute array of corresponding error probabilities\n",
    "def error_prob_vs_delta(n=100000, eps=0.4, min_delta=0.01, max_delta=0.1):\n",
    "  deltas = np.linspace(min_delta, max_delta, 10).tolist()\n",
    "  # deltas = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "  ps = []\n",
    "  for delta in deltas:\n",
    "    probs = 0\n",
    "    # Average probabilities across 3 trials\n",
    "    for i in range(3):\n",
    "      cms = CountMinSketch(eps, delta)\n",
    "      dt = generate_sample(n)\n",
    "      load_data(cms, dt)\n",
    "      p, avg_err, max_err, err = compute_error_prob(cms, dt, n)\n",
    "      probs += p\n",
    "    probs /= 3\n",
    "    ps.append(probs)\n",
    "  return deltas, ps\n",
    "\n",
    "# graphing helper function\n",
    "def graph_error_prob_vs_delta(deltas, ps, filename=\"p_vs_delta.png\"):\n",
    "  plt.scatter(deltas, ps)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "code_folding": [
     0,
     6,
     9,
     29
    ]
   },
   "outputs": [],
   "source": [
    "def generate_new_sample(eps, delta, distribution, n): \n",
    "    global sample\n",
    "    sample = generate_sample(n=n, dist=distribution)\n",
    "    return sample\n",
    "\n",
    "# COUNT MIN ACROSS DISTS\n",
    "def get_sample(eps, delta, distribution, n): \n",
    "    return generate_sample(n=n, dist=distribution)\n",
    "\n",
    "def get_y_errors(xs, n, eps, delta, distribution):\n",
    "    n = n\n",
    "    threshold = eps * n\n",
    "\n",
    "    mean = 0\n",
    "    sd = 100\n",
    "    sample = xs\n",
    "\n",
    "    cms = CountMinSketch(eps, delta)\n",
    "    load_data(cms, sample)\n",
    "\n",
    "    p, avg_err, max_err, err = compute_error_prob(cms, sample, n)\n",
    "\n",
    "    print(\"Average Error: \" + str(avg_err))\n",
    "    print(\"Maximum Error: \" + str(max_err))\n",
    "    print(\"Acceptable Threshold: \" + str(threshold))\n",
    "    print(\"Proportion of Errors Exceeding Threshold: \" + str(p))\n",
    "\n",
    "    return err\n",
    "\n",
    "def get_data_for_hist_errors(n, eps, delta, distribution):\n",
    "    n = n\n",
    "    threshold = eps * n\n",
    "\n",
    "    mean = 0\n",
    "    sd = 100\n",
    "    sample = generate_new_sample(eps, delta, distribution, n)\n",
    "    cms = CountMinSketch(eps, delta)\n",
    "    load_data(cms, sample)\n",
    "\n",
    "    p, avg_err, max_err, err = compute_error_prob(cms, sample, n)\n",
    "\n",
    "    print(\"Average Error: \" + str(avg_err))\n",
    "    print(\"Maximum Error: \" + str(max_err))\n",
    "    print(\"Acceptable Threshold: \" + str(threshold))\n",
    "    print(\"Proportion of Errors Exceeding Threshold: \" + str(p))\n",
    "\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-Min Sketch estimates across distributions \n",
    "The plots below shows a scatterplot of errors with data points (left), the distribution of data (center), and the distribution of errors (right). Again, we can see that the Count-Min Sketch gives us decent estimates; none of the errors exceed the threshold, which is excellent! But interestingly, by converting to a normal distribution, our average error almost halved, from 20.154 to 11.4482, and our max error shot up by 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c08895e4744928aee004b8a55b547d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(interactive(children=(IntSlider(value=1000, description='n', max=2000, min=1), FloatSlider(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opts = {\n",
    "    'title': \"Scatterplot of Errors\",\n",
    "    'ylabel': \"Error\",\n",
    "    'xlabel': \"Data Point\",\n",
    "    'ylim': (0, 500),}\n",
    "nbi.scatter(generate_hist_sample, get_y_errors, n=(1,2000), eps=(0.01,1, 0.01), delta=(0.01, 1, 0.01), distribution={'normal': \"normal\", 'zipf': 'zipf', 'uniform': 'uniform', 'exp':'exp', 'geometric':'geometric', 'lognorm':'lognorm'}, options=opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "code_folding": [
     0,
     22
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ffe549f1be41dda150159b03b32240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(interactive(children=(IntSlider(value=5000, description='n', max=10000, min=1), Dropdown(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_hist_sample(n, distribution, loc=0, scale=10000, lambda_=5, s=1, a=6.5):\n",
    "    dist = distribution\n",
    "    if dist == 'uniform':\n",
    "        float_sample = stats.uniform.rvs(loc, scale, n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'zipf':\n",
    "        float_sample = stats.zipf.rvs(a, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'exp':\n",
    "        float_sample = stats.planck.rvs(lambda_, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'lognorm':\n",
    "        float_sample = stats.lognorm.rvs(s=s, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'geometric':\n",
    "        float_sample =  stats.geom.rvs(p, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    elif dist == 'normal':\n",
    "        float_sample = stats.norm.rvs(loc, scale, n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    else:\n",
    "        return -1    \n",
    "opts = {\n",
    "    'title': \"Distribution of Data\",\n",
    "    'ylabel': \"Count\",\n",
    "    'xlabel': \"Data Point\",}\n",
    "nbi.hist(generate_hist_sample, n=(1,10000), distribution={'normal': \"normal\", 'zipf': 'zipf', 'uniform': 'uniform', 'exp':'exp', 'geometric':'geometric', 'lognorm':'lognorm'}, options=opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f6c9d06af24e81aa5e7c8d77a3dbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(interactive(children=(IntSlider(value=1000, description='n', max=2000, min=1), FloatSlider(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opts = {\n",
    "    'title': \"Distribution of errors\",\n",
    "    'ylabel': \"Count\",\n",
    "    'xlabel': \"Error Magnitude\",}\n",
    "nbi.hist(get_data_for_hist_errors, n=(1,2000), eps=(0.01,1, 0.01), delta=(0.01, 1, 0.01), distribution={'normal': \"normal\", 'zipf': 'zipf', 'uniform': 'uniform', 'exp':'exp', 'geometric':'geometric', 'lognorm':'lognorm'}, options=opts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does data spread affect our errors?\n",
    "Let’s take this a step further and scope out the exact effect on standard deviation (a mathematical proxy for the “spread” of the data) on the errors. Let’s investigate what happens when we sample n = 1000 integers from a normal distribution with a mean of μ = 0. We vary the standard deviation between 1 and 100 to understand how spreading the distribution affects errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "code_folding": [
     5,
     7,
     26
    ]
   },
   "outputs": [],
   "source": [
    "# COUNT MIN ACROSS SDS\n",
    "def get_sample_sd(eps, delta, n, sd): \n",
    "    return generate_sample(n=n, dist=\"normal\", scale=sd)\n",
    "def get_y_errors_sd(xs, n, eps, delta, sd):\n",
    "    n = n\n",
    "    threshold = eps * n\n",
    "\n",
    "    mean = 0\n",
    "    sd = 100\n",
    "    sample = xs\n",
    "\n",
    "    cms = CountMinSketch(eps, delta)\n",
    "    load_data(cms, sample)\n",
    "\n",
    "    p, avg_err, max_err, err = compute_error_prob(cms, sample, n)\n",
    "\n",
    "    print(\"Average Error: \" + str(avg_err))\n",
    "    print(\"Maximum Error: \" + str(max_err))\n",
    "    print(\"Acceptable Threshold: \" + str(threshold))\n",
    "    print(\"Proportion of Errors Exceeding Threshold: \" + str(p))\n",
    "\n",
    "    return err\n",
    "def get_data_for_hist_errors_sd(n, eps, delta, sd):\n",
    "    n = n\n",
    "    threshold = eps * n\n",
    "\n",
    "    mean = 0\n",
    "    sd = 100\n",
    "    sample = get_sample_sd(eps, delta, n, sd)\n",
    "    cms = CountMinSketch(eps, delta)\n",
    "    load_data(cms, sample)\n",
    "\n",
    "    p, avg_err, max_err, err = compute_error_prob(cms, sample, n)\n",
    "\n",
    "    print(\"Average Error: \" + str(avg_err))\n",
    "    print(\"Maximum Error: \" + str(max_err))\n",
    "    print(\"Acceptable Threshold: \" + str(threshold))\n",
    "    print(\"Proportion of Errors Exceeding Threshold: \" + str(p))\n",
    "\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'title': \"Scatterplot of Errors\",\n",
    "    'ylabel': \"Error\",\n",
    "    'xlabel': \"Data Point\",\n",
    "    'ylim': (0, 500),}\n",
    "print(\"Vary the standard distribution to see how the errors change!\")\n",
    "\n",
    "nbi.scatter(get_sample_sd, get_y_errors_sd, n=(1,2000), eps=(0.01,1, 0.01), delta=(0.01, 1, 0.01), sd=(0.01, 1000, 10), options=opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vary the standard distribution to see how error disribution change!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023c1908780040bfa658d47df9782635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(interactive(children=(IntSlider(value=1000, description='n', max=2000, min=1), FloatSlider(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opts = {\n",
    "    'title': \"Distribution of errors\",\n",
    "    'ylabel': \"Count\",\n",
    "    'xlabel': \"Error Magnitude\",}\n",
    "print(\"Vary the standard distribution to see how error disribution change!\")\n",
    "nbi.hist(get_data_for_hist_errors_sd, n=(1,2000), eps=(0.01,1, 0.01), delta=(0.01, 1, 0.01),sd=(0.01, 1000, 10),  options=opts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization #1: The Learned Count-Min Sketch\n",
    "\n",
    "One approach is simply to treat the heavy hitters and non-heavy-hitters separately. This is where we can motivate our data structure design with two ideas from the original Learned Index Structures paper –– recursive models and auxiliary structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LearnedCountMinSketch:\n",
    "    def __init__(self, eps, delta, train_data):\n",
    "        self.eps = eps\n",
    "        self.delta = delta\n",
    "        self.cms = CountMinSketch(eps, delta)\n",
    "\n",
    "        # set model\n",
    "        X_train = train_data[0]\n",
    "        Y_train = train_data[1]\n",
    "        self.model = MLPClassifier(hidden_layer_sizes=(30, 40))\n",
    "        self.model.fit(X_train.reshape(-1, 1), np.ravel(Y_train))\n",
    "        self.perfect = {}\n",
    "\n",
    "    def count(self, value):\n",
    "        if (self.model.predict(np.array([value]).reshape(-1, 1)) == 1):\n",
    "            if str(value) in self.perfect:\n",
    "                self.perfect[str(value)] = self.perfect[str(value)] + 1\n",
    "            else:\n",
    "                self.perfect[str(value)] = 1\n",
    "        else:\n",
    "            self.cms.count(value)\n",
    "\n",
    "    def estimate(self, value):\n",
    "        if (self.model.predict(np.array([value]).reshape(-1, 1)) == 1):\n",
    "            if str(value) in self.perfect: return self.perfect[str(value)]\n",
    "            return 0\n",
    "        else:\n",
    "            return self.cms.estimate(value)\n",
    "\n",
    "    def real_estimate(self, value):\n",
    "        if str(value) in self.perfect: return self.perfect[str(value)]\n",
    "        if str(value) in self.cms.backup: return self.cms.backup[str(value)]\n",
    "        return -1\n",
    "\n",
    "    def compute_size(self):\n",
    "        size = 0\n",
    "        for key in self.cms.backup:\n",
    "            size += abs(self.cms.backup[key])\n",
    "        for key in self.perfect:\n",
    "            size += abs(self.perfect[key])\n",
    "        return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# label data --> threshold is what proportion of data points you want to call \"heavy hitters\"\n",
    "def label_sample(sample, p = 0.05):\n",
    "    n = len(sample)\n",
    "    n_distinct = len(Counter(sample).keys())\n",
    "    num = int(n_distinct * p)\n",
    "    X_train = np.array(sample)\n",
    "    Y_train = np.zeros_like(X_train)\n",
    "    hh = Counter(sample).most_common(num)\n",
    "    hh = set([el[0] for el in hh])\n",
    "    for i in range(n):\n",
    "        if X_train[i] in hh:\n",
    "            Y_train[i] = 1\n",
    "    return X_train, Y_train, hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "code_folding": [
     0,
     168,
     175,
     183,
     187,
     190,
     201,
     205,
     274
    ]
   },
   "outputs": [],
   "source": [
    "# VISUALIZE ML LEARNED COUNT MIN SKETCH\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import json \n",
    "\n",
    "class CountMinSketch:\n",
    "    def __init__(self, eps, delta):\n",
    "        self.eps = eps\n",
    "        self.delta = delta\n",
    "        self.w = math.ceil(np.exp(1) / eps)\n",
    "        self.d = math.ceil(np.log(1 / delta))\n",
    "        self.tables = np.zeros((self.d, self.w))\n",
    "        self.backup = {}\n",
    "\n",
    "    def compute_hash(self, value, table_no):\n",
    "        fn = hashlib.md5()\n",
    "        inp = str(value) + str(0) + str(table_no)\n",
    "        fn.update(inp.encode())\n",
    "        out = int(fn.hexdigest(), 16)\n",
    "        return out % self.w\n",
    "\n",
    "    def count(self, value):\n",
    "        if str(value) in self.backup: \n",
    "            self.backup[str(value)] = self.backup[str(value)] + 1\n",
    "        else:\n",
    "            self.backup[str(value)] = 1\n",
    "        for i in range(self.d):\n",
    "            j = self.compute_hash(value, i)\n",
    "            self.tables[i][j] = self.tables[i][j] + 1\n",
    "\n",
    "    def estimate(self, value):\n",
    "        ests = []\n",
    "        for i in range(self.d):\n",
    "            j = self.compute_hash(value, i)\n",
    "            ests.append(self.tables[i][j])\n",
    "        return min(ests)\n",
    "\n",
    "    def real_estimate(self, value):\n",
    "        if str(value) in self.backup: return self.backup[str(value)]\n",
    "        return -1\n",
    "\n",
    "    def compute_size(self):\n",
    "        size = 0\n",
    "        for key in self.backup:\n",
    "            size += abs(self.backup[key])\n",
    "        return size\n",
    "\n",
    "    def save_counts(self, count_filename='counts.txt', actual_filename='backups.txt'):\n",
    "        np.savetxt(count_filename, self.tables)\n",
    "        with open(actual_filename, 'w') as fp: json.dump(self.backup, fp)\n",
    "\n",
    "    def load_counts(self, count_filename='counts.txt', actual_filename='backups.txt'):\n",
    "        with open(actual_filename, 'r') as fp: \n",
    "            temp = json.load(fp)\n",
    "            self.backup = temp\n",
    "        self.tables = np.loadtxt(count_filename)\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "def generate_sample(n=1000, dist='uniform', loc=0, scale=1000, lambda_=5, s=1):\n",
    "  if dist == 'uniform':\n",
    "    float_sample = stats.uniform.rvs(loc, scale, n)\n",
    "    return [int(el) for el in float_sample]\n",
    "  if dist == 'zipf':\n",
    "    float_sample = stats.zipf.rvs(loc + 1, size=n)\n",
    "    return [int(el) for el in float_sample]\n",
    "  if dist == 'exp':\n",
    "    float_sample = stats.planck.rvs(lambda_, size=n)\n",
    "    return [int(el) for el in float_sample]\n",
    "  if dist == 'lognorm':\n",
    "    float_sample = stats.lognorm.rvs(s=scale, size=n)\n",
    "    return [int(el) for el in float_sample]\n",
    "  if dist == 'geometric':\n",
    "    float_sample =  stats.geom.rvs(p, size=n)\n",
    "    return [int(el) for el in float_sample]\n",
    "  elif dist == 'normal':\n",
    "    float_sample = stats.norm.rvs(loc, scale, n)\n",
    "    return [int(el) for el in float_sample]\n",
    "  else:\n",
    "    return -1\n",
    "\n",
    "for el in uniform_sample:\n",
    "  cms.count(el)\n",
    "\n",
    "def compute_error_prob(cms, data, n):\n",
    "  err = []\n",
    "  for el in data:\n",
    "    err.append(cms.estimate(el) - cms.real_estimate(el))\n",
    "  avg_err = sum(err) / len(err)\n",
    "  max_err = max(err)\n",
    "  exceed = 0\n",
    "  for el in err:\n",
    "    if el > cms.eps * n:\n",
    "      exceed += 1\n",
    "  return exceed / len(err), err\n",
    "\n",
    "def load_data(cms, data):\n",
    "  for el in data:\n",
    "    cms.count(el)\n",
    "\n",
    "def run_experiment():\n",
    "  n = 1000\n",
    "  eps = 0.01\n",
    "  min_delta = 0.01\n",
    "  max_delta = 0.1\n",
    "  deltas = np.linspace(min_delta, max_delta, 10).tolist()\n",
    "  # deltas = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "  ps = []\n",
    "  for delta in deltas:\n",
    "    probs = 0\n",
    "    for i in range(3):\n",
    "      cms = CountMinSketch(eps, delta)\n",
    "      print(cms.w)\n",
    "      dt = generate_sample(n)\n",
    "      load_data(cms, dt)\n",
    "      p, err = compute_error_prob(cms, dt, n)\n",
    "      print(err)\n",
    "      probs += p\n",
    "    probs /= 3\n",
    "    ps.append(probs)\n",
    "  return deltas, ps, err\n",
    "\n",
    "# VISUALIZE ERROR PROBABILITY VS DELTA\n",
    "\n",
    "# compute empirical probability of error exceeding the threshold\n",
    "def compute_error_prob_ml(cms, data, n):\n",
    "  err = []\n",
    "  for el in data:\n",
    "    err.append(cms.estimate(el) - cms.real_estimate(el))\n",
    "  avg_err = sum(err) / len(err)\n",
    "  max_err = max(err)\n",
    "  exceed = 0\n",
    "  for el in err:\n",
    "    if el > cms.eps * n:\n",
    "      exceed += 1\n",
    "  p = exceed / len(err)\n",
    "  return p, avg_err, max_err, err\n",
    "\n",
    "# run experiments on 10 values of delta interpolated between (min_delta, max_delta) and compute array of corresponding error probabilities\n",
    "def error_prob_vs_delta(n=100000, eps=0.4, min_delta=0.01, max_delta=0.1):\n",
    "  deltas = np.linspace(min_delta, max_delta, 10).tolist()\n",
    "  # deltas = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "  ps = []\n",
    "  for delta in deltas:\n",
    "    probs = 0\n",
    "    # Average probabilities across 3 trials\n",
    "    for i in range(3):\n",
    "      cms = CountMinSketch(eps, delta)\n",
    "      dt = generate_sample(n)\n",
    "      load_data(cms, dt)\n",
    "      p, avg_err, max_err, err = compute_error_prob(cms, dt, n)\n",
    "      probs += p\n",
    "    probs /= 3\n",
    "    ps.append(probs)\n",
    "  return deltas, ps\n",
    "\n",
    "# graphing helper function\n",
    "def graph_error_prob_vs_delta(deltas, ps, filename=\"p_vs_delta.png\"):\n",
    "  plt.scatter(deltas, ps)\n",
    "  plt.show()\n",
    "\n",
    "    \n",
    "# compute empirical probability of error exceeding the threshold\n",
    "def compute_error_prob(cms, data, n):\n",
    "  err = []\n",
    "  for el in data:\n",
    "    err.append(cms.estimate(el) - cms.real_estimate(el))\n",
    "  avg_err = sum(err) / len(err)\n",
    "  max_err = max(err)\n",
    "  exceed = 0\n",
    "  for el in err:\n",
    "    if el > cms.eps * n:\n",
    "      exceed += 1\n",
    "  p = exceed / len(err)\n",
    "  return p, avg_err, max_err, err\n",
    "  #return err\n",
    "\n",
    "# run experiments on 10 values of delta interpolated between (min_delta, max_delta) and compute array of corresponding error probabilities\n",
    "def error_prob_vs_delta(n=100000, eps=0.4, min_delta=0.01, max_delta=0.1):\n",
    "  deltas = np.linspace(min_delta, max_delta, 10).tolist()\n",
    "  # deltas = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "  ps = []\n",
    "  for delta in deltas:\n",
    "    probs = 0\n",
    "    # Average probabilities across 3 trials\n",
    "    for i in range(3):\n",
    "      cms = CountMinSketch(eps, delta)\n",
    "      dt = generate_sample(n)\n",
    "      load_data(cms, dt)\n",
    "      p, avg_err, max_err, err = compute_error_prob(cms, dt, n)\n",
    "      probs += p\n",
    "    probs /= 3\n",
    "    ps.append(probs)\n",
    "  return deltas, ps\n",
    "\n",
    "# graphing helper function\n",
    "def graph_error_prob_vs_delta(deltas, ps, filename=\"p_vs_delta.png\"):\n",
    "  plt.scatter(deltas, ps)\n",
    "  plt.show()\n",
    "    \n",
    "def opt_1_normal(sd):\n",
    "    eps = 0.01\n",
    "    delta = 0.05\n",
    "    n = 1000\n",
    "    threshold = eps * n\n",
    "\n",
    "    p = 0.2\n",
    "    sample = generate_sample(n=n, dist='normal', loc=mean, scale=sd)\n",
    "    X_tr, Y_tr, hh = label_sample(sample, p)\n",
    "\n",
    "    cms = CountMinSketch(eps, delta)\n",
    "\n",
    "    load_data(cms, sample)\n",
    "    p, avg_err, max_err, err = compute_error_prob(cms, sample, n)\n",
    "    return err\n",
    "\n",
    "def opt_1_learned(sd):\n",
    "    eps = 0.01\n",
    "    delta = 0.05\n",
    "    n = 1000\n",
    "    threshold = eps * n\n",
    "\n",
    "    p = 0.2\n",
    "    sample = generate_sample(n=n, dist='normal', loc=mean, scale=sd)\n",
    "    X_tr, Y_tr, hh = label_sample(sample, p)\n",
    "\n",
    "    lcms = LearnedCountMinSketch(eps, delta, [X_tr, Y_tr])\n",
    "\n",
    "    load_data(lcms, sample)\n",
    "    p, avg_err, max_err, err = compute_error_prob(lcms, sample, n)\n",
    "    return err\n",
    "\n",
    "class RuleCountMinSketch:\n",
    "    def __init__(self, eps, delta, hh):\n",
    "        self.eps = eps\n",
    "        self.delta = delta\n",
    "        self.cms = CountMinSketch(eps, delta)\n",
    "        self.hh = hh\n",
    "        self.perfect = {}\n",
    "\n",
    "    def count(self, value):\n",
    "        if value in self.hh:\n",
    "            if str(value) in self.perfect:\n",
    "                self.perfect[str(value)] = self.perfect[str(value)] + 1\n",
    "            else:\n",
    "                self.perfect[str(value)] = 1\n",
    "        else:\n",
    "            self.cms.count(value)\n",
    "\n",
    "    def estimate(self, value):\n",
    "        if (value in self.hh):\n",
    "            if str(value) in self.perfect: return self.perfect[str(value)]\n",
    "            return 0\n",
    "        else:\n",
    "            return self.cms.estimate(value)\n",
    "\n",
    "    def real_estimate(self, value):\n",
    "        if str(value) in self.perfect: return self.perfect[str(value)]\n",
    "        if str(value) in self.cms.backup: return self.cms.backup[str(value)]\n",
    "        return -1\n",
    "\n",
    "    def compute_size(self):\n",
    "        size = 0\n",
    "        for key in self.cms.backup:\n",
    "            size += abs(self.cms.backup[key])\n",
    "        for key in self.perfect:\n",
    "            size += abs(self.perfect[key])\n",
    "        return size\n",
    "\n",
    "def opt_2(sd):\n",
    "    eps = 0.01\n",
    "    delta = 0.05\n",
    "    n = 1000\n",
    "    threshold = eps * n\n",
    "\n",
    "    p = 0.2\n",
    "    sample = generate_sample(n=n, dist='normal', loc=mean, scale=sd)\n",
    "    X_tr, Y_tr, hh = label_sample(sample, p)\n",
    "    rcms = RuleCountMinSketch(eps, delta, hh)\n",
    "\n",
    "    load_data(rcms, sample)\n",
    "    p, avg_err, max_err, err = compute_error_prob(rcms, sample, n)\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vary the standard distribution to see how error disribution change!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455c8182d99647e795bbdceaf9603528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(interactive(children=(IntSlider(value=500, description='sd', max=1000, step=10), Output()), _do…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opts = {\n",
    "    'title': \"Count-Min Sketch\",\n",
    "    'ylabel': \"Frequency\",\n",
    "    'xlabel': \"Error\",}\n",
    "print(\"Vary the standard distribution to see how error disribution change!\")\n",
    "nbi.hist(opt_1_normal, sd=(0, 1000, 10),  options=opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vary the standard distribution to see how error disribution change!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd328685ad6f413a98c9b89f4f4d68f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(interactive(children=(IntSlider(value=500, description='sd', max=1000, step=10), Output()), _do…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opts = {\n",
    "    'title': \"Learned Count-Min Sketch\",\n",
    "    'ylabel': \"Frequency\",\n",
    "    'xlabel': \"Error\",}\n",
    "print(\"Vary the standard distribution to see how error disribution change!\")\n",
    "nbi.hist(opt_1_normal, sd=(0, 1000, 10),  options=opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vary the standard distribution to see how error disribution change!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cffd00f6d8547bb844e5998bae30c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(interactive(children=(IntSlider(value=500, description='sd', max=1000, step=10), Output()), _do…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opts = {\n",
    "    'title': \"Rule Based Count-Min Sketch\",\n",
    "    'ylabel': \"Frequency\",\n",
    "    'xlabel': \"Error\",}\n",
    "print(\"Vary the standard distribution to see how error disribution change!\")\n",
    "nbi.hist(opt_2, sd=(0, 1000, 10),  options=opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/markiewagner/Desktop/cs166-final-proj\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbi.publish('markiewagner/cs166-proj-final/master', 'demo.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
