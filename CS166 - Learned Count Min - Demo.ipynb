{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scipy) (1.15.4)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sklearn in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from sklearn) (0.21.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.15.4)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nbinteract in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.2.5)\n",
      "Requirement already satisfied: nbformat<5,>=4.4.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbinteract) (4.4.0)\n",
      "Requirement already satisfied: ipywidgets<8,>=7.5.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbinteract) (7.5.1)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbinteract) (1.15.4)\n",
      "Requirement already satisfied: IPython<8,>=6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbinteract) (7.5.0)\n",
      "Requirement already satisfied: traitlets<5,>=4.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbinteract) (4.3.2)\n",
      "Requirement already satisfied: toolz<1,>=0.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbinteract) (0.10.0)\n",
      "Requirement already satisfied: nbconvert<6,>=5.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbinteract) (5.6.0)\n",
      "Requirement already satisfied: Jinja2<3,>=2.10 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbinteract) (2.11.1)\n",
      "Requirement already satisfied: bqplot==0.11.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbinteract) (0.11.0)\n",
      "Requirement already satisfied: docopt<1,>=0.6.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbinteract) (0.6.2)\n",
      "Requirement already satisfied: jupyter-core in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbformat<5,>=4.4.0->nbinteract) (4.6.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbformat<5,>=4.4.0->nbinteract) (3.0.2)\n",
      "Requirement already satisfied: ipython-genutils in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbformat<5,>=4.4.0->nbinteract) (0.2.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from ipywidgets<8,>=7.5.0->nbinteract) (5.1.4)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from ipywidgets<8,>=7.5.0->nbinteract) (3.5.1)\n",
      "Requirement already satisfied: pygments in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from IPython<8,>=6->nbinteract) (2.4.2)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from IPython<8,>=6->nbinteract) (0.1.0)\n",
      "Requirement already satisfied: decorator in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from IPython<8,>=6->nbinteract) (4.4.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from IPython<8,>=6->nbinteract) (40.6.2)\n",
      "Requirement already satisfied: backcall in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from IPython<8,>=6->nbinteract) (0.1.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from IPython<8,>=6->nbinteract) (2.0.9)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from IPython<8,>=6->nbinteract) (4.7.0)\n",
      "Requirement already satisfied: pickleshare in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from IPython<8,>=6->nbinteract) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from IPython<8,>=6->nbinteract) (0.13.3)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from traitlets<5,>=4.3->nbinteract) (1.15.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbconvert<6,>=5.3->nbinteract) (0.3)\n",
      "Requirement already satisfied: bleach in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbconvert<6,>=5.3->nbinteract) (3.1.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbconvert<6,>=5.3->nbinteract) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbconvert<6,>=5.3->nbinteract) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbconvert<6,>=5.3->nbinteract) (0.6.0)\n",
      "Requirement already satisfied: testpath in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nbconvert<6,>=5.3->nbinteract) (0.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from Jinja2<3,>=2.10->nbinteract) (1.1.0)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from bqplot==0.11.0->nbinteract) (0.24.2)\n",
      "Requirement already satisfied: traittypes>=0.0.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from bqplot==0.11.0->nbinteract) (0.2.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat<5,>=4.4.0->nbinteract) (19.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat<5,>=4.4.0->nbinteract) (0.15.4)\n",
      "Requirement already satisfied: jupyter-client in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.5.0->nbinteract) (5.3.4)\n",
      "Requirement already satisfied: tornado>=4.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.5.0->nbinteract) (5.1.1)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7.5.0->nbinteract) (6.0.1)\n",
      "Requirement already satisfied: wcwidth in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython<8,>=6->nbinteract) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->IPython<8,>=6->nbinteract) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from jedi>=0.10->IPython<8,>=6->nbinteract) (0.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webencodings in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from bleach->nbconvert<6,>=5.3->nbinteract) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas->bqplot==0.11.0->nbinteract) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas->bqplot==0.11.0->nbinteract) (2019.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<8,>=7.5.0->nbinteract) (18.0.1)\n",
      "Requirement already satisfied: prometheus-client in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.5.0->nbinteract) (0.7.1)\n",
      "Requirement already satisfied: Send2Trash in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.5.0->nbinteract) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.5.0->nbinteract) (0.8.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scipy\n",
    "!pip3 install sklearn\n",
    "!pip3 install nbinteract\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import math\n",
    "import hashlib\n",
    "import scipy.stats as stats\n",
    "import json \n",
    "import nbinteract as nbi\n",
    "from sklearn import svm\n",
    "from collections import Counter\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     1,
     52,
     55
    ]
   },
   "outputs": [],
   "source": [
    "class CountMinSketch:\n",
    "    def __init__(self, eps, delta):\n",
    "        self.eps = eps\n",
    "        self.delta = delta\n",
    "        self.w = math.ceil(np.exp(1) / eps)\n",
    "        self.d = math.ceil(np.log(1 / delta))\n",
    "        self.tables = np.zeros((self.d, self.w))\n",
    "        self.backup = {}\n",
    "\n",
    "    def compute_hash(self, value, table_no):\n",
    "        fn = hashlib.md5()\n",
    "        inp = str(value) + str(0) + str(table_no)\n",
    "        fn.update(inp.encode())\n",
    "        out = int(fn.hexdigest(), 16)\n",
    "        return out % self.w\n",
    "\n",
    "    def count(self, value):\n",
    "        if str(value) in self.backup: \n",
    "            self.backup[str(value)] = self.backup[str(value)] + 1\n",
    "        else:\n",
    "            self.backup[str(value)] = 1\n",
    "        for i in range(self.d):\n",
    "            j = self.compute_hash(value, i)\n",
    "            self.tables[i][j] = self.tables[i][j] + 1\n",
    "\n",
    "    def estimate(self, value):\n",
    "        ests = []\n",
    "        for i in range(self.d):\n",
    "            j = self.compute_hash(value, i)\n",
    "            ests.append(self.tables[i][j])\n",
    "        return min(ests)\n",
    "\n",
    "    def real_estimate(self, value):\n",
    "        if str(value) in self.backup: return self.backup[str(value)]\n",
    "        return -1\n",
    "\n",
    "    def compute_size(self):\n",
    "        size = 0\n",
    "        for key in self.backup:\n",
    "            size += abs(self.backup[key])\n",
    "        return size\n",
    "\n",
    "    def save_counts(self, count_filename='counts.txt', actual_filename='backups.txt'):\n",
    "        np.savetxt(count_filename, self.tables)\n",
    "        with open(actual_filename, 'w') as fp: json.dump(self.backup, fp)\n",
    "\n",
    "    def load_counts(self, count_filename='counts.txt', actual_filename='backups.txt'):\n",
    "        with open(actual_filename, 'r') as fp: \n",
    "            temp = json.load(fp)\n",
    "            self.backup = temp\n",
    "        self.tables = np.loadtxt(count_filename)\n",
    "        \n",
    "def load_data(cms, data):\n",
    "  for el in data:\n",
    "    cms.count(el)\n",
    "def generate_sample(n=1000, dist='uniform', loc=0, scale=100, lambda_=5, s=100):\n",
    "    if dist == 'uniform':\n",
    "        float_sample = stats.uniform.rvs(loc, scale, n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'zipf':\n",
    "        float_sample = stats.zipf.rvs(a, size=n)\n",
    "        return [(el) for el in float_sample]\n",
    "    if dist == 'exp':\n",
    "        float_sample = planck.rvs(lambda_, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'lognorm':\n",
    "        float_sample = lognorm.rvs(s=s, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'geometric':\n",
    "        float_sample =  geom.rvs(p, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    elif dist == 'normal':\n",
    "        float_sample = stats.norm.rvs(loc, scale, n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     3,
     17,
     35
    ]
   },
   "outputs": [],
   "source": [
    "# VISUALIZE ERROR PROBABILITY VS DELTA\n",
    "\n",
    "# compute empirical probability of error exceeding the threshold\n",
    "def compute_error_prob(cms, data, n):\n",
    "  err = []\n",
    "  for el in data:\n",
    "    err.append(cms.estimate(el) - cms.real_estimate(el))\n",
    "  avg_err = sum(err) / len(err)\n",
    "  max_err = max(err)\n",
    "  exceed = 0\n",
    "  for el in err:\n",
    "    if el > cms.eps * n:\n",
    "      exceed += 1\n",
    "  p = exceed / len(err)\n",
    "  return p, avg_err, max_err, err\n",
    "\n",
    "# run experiments on 10 values of delta interpolated between (min_delta, max_delta) and compute array of corresponding error probabilities\n",
    "def error_prob_vs_delta(n=100000, eps=0.4, min_delta=0.01, max_delta=0.1):\n",
    "  deltas = np.linspace(min_delta, max_delta, 10).tolist()\n",
    "  # deltas = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "  ps = []\n",
    "  for delta in deltas:\n",
    "    probs = 0\n",
    "    # Average probabilities across 3 trials\n",
    "    for i in range(3):\n",
    "      cms = CountMinSketch(eps, delta)\n",
    "      dt = generate_sample(n)\n",
    "      load_data(cms, dt)\n",
    "      p, avg_err, max_err, err = compute_error_prob(cms, dt, n)\n",
    "      probs += p\n",
    "    probs /= 3\n",
    "    ps.append(probs)\n",
    "  return deltas, ps\n",
    "\n",
    "# graphing helper function\n",
    "def graph_error_prob_vs_delta(deltas, ps, filename=\"p_vs_delta.png\"):\n",
    "  plt.scatter(deltas, ps)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     6,
     9,
     29
    ]
   },
   "outputs": [],
   "source": [
    "def generate_new_sample(eps, delta, distribution, n): \n",
    "    global sample\n",
    "    sample = generate_sample(n=n, dist=distribution)\n",
    "    return sample\n",
    "\n",
    "# COUNT MIN ACROSS DISTS\n",
    "def get_sample(eps, delta, distribution, n): \n",
    "    return generate_sample(n=n, dist=distribution)\n",
    "\n",
    "def get_y_errors(xs, n, eps, delta, distribution):\n",
    "    n = n\n",
    "    threshold = eps * n\n",
    "\n",
    "    mean = 0\n",
    "    sd = 100\n",
    "    sample = xs\n",
    "\n",
    "    cms = CountMinSketch(eps, delta)\n",
    "    load_data(cms, sample)\n",
    "\n",
    "    p, avg_err, max_err, err = compute_error_prob(cms, sample, n)\n",
    "\n",
    "    print(\"Average Error: \" + str(avg_err))\n",
    "    print(\"Maximum Error: \" + str(max_err))\n",
    "    print(\"Acceptable Threshold: \" + str(threshold))\n",
    "    print(\"Proportion of Errors Exceeding Threshold: \" + str(p))\n",
    "\n",
    "    return err\n",
    "\n",
    "def get_data_for_hist_errors(n, eps, delta, distribution):\n",
    "    n = n\n",
    "    threshold = eps * n\n",
    "\n",
    "    mean = 0\n",
    "    sd = 100\n",
    "    sample = generate_new_sample(eps, delta, distribution, n)\n",
    "    cms = CountMinSketch(eps, delta)\n",
    "    load_data(cms, sample)\n",
    "\n",
    "    p, avg_err, max_err, err = compute_error_prob(cms, sample, n)\n",
    "\n",
    "    print(\"Average Error: \" + str(avg_err))\n",
    "    print(\"Maximum Error: \" + str(max_err))\n",
    "    print(\"Acceptable Threshold: \" + str(threshold))\n",
    "    print(\"Proportion of Errors Exceeding Threshold: \" + str(p))\n",
    "\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-Min Sketch estimates across distributions \n",
    "The plots below shows a scatterplot of errors with data points (left), the distribution of data (center), and the distribution of errors (right). Again, we can see that the Count-Min Sketch gives us decent estimates; none of the errors exceed the threshold, which is excellent! But interestingly, by converting to a normal distribution, our average error almost halved, from 20.154 to 11.4482, and our max error shot up by 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_hist_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bd1f8f91240e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'xlabel'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Data Point\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     'ylim': (0, 500),}\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnbi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_hist_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_y_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'normal'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"normal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zipf'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'zipf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'uniform'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'exp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'geometric'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'geometric'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lognorm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'lognorm'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_hist_sample' is not defined"
     ]
    }
   ],
   "source": [
    "opts = {\n",
    "    'title': \"Scatterplot of Errors\",\n",
    "    'ylabel': \"Error\",\n",
    "    'xlabel': \"Data Point\",\n",
    "    'ylim': (0, 500),}\n",
    "nbi.scatter(generate_hist_sample, get_y_errors, n=(1,2000), eps=(0.01,1, 0.01), delta=(0.01, 1, 0.01), distribution={'normal': \"normal\", 'zipf': 'zipf', 'uniform': 'uniform', 'exp':'exp', 'geometric':'geometric', 'lognorm':'lognorm'}, options=opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_hist_sample(n, distribution, loc=0, scale=10000, lambda_=5, s=1, a=6.5):\n",
    "    dist = distribution\n",
    "    if dist == 'uniform':\n",
    "        float_sample = stats.uniform.rvs(loc, scale, n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'zipf':\n",
    "        float_sample = stats.zipf.rvs(a, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'exp':\n",
    "        float_sample = stats.planck.rvs(lambda_, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'lognorm':\n",
    "        float_sample = stats.lognorm.rvs(s=s, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    if dist == 'geometric':\n",
    "        float_sample =  stats.geom.rvs(p, size=n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    elif dist == 'normal':\n",
    "        float_sample = stats.norm.rvs(loc, scale, n)\n",
    "        return [int(el) for el in float_sample]\n",
    "    else:\n",
    "        return -1    \n",
    "opts = {\n",
    "    'title': \"Distribution of Data\",\n",
    "    'ylabel': \"Count\",\n",
    "    'xlabel': \"Data Point\",}\n",
    "nbi.hist(generate_hist_sample, n=(1,10000), distribution={'normal': \"normal\", 'zipf': 'zipf', 'uniform': 'uniform', 'exp':'exp', 'geometric':'geometric', 'lognorm':'lognorm'}, options=opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'title': \"Distribution of errors\",\n",
    "    'ylabel': \"Count\",\n",
    "    'xlabel': \"Error Magnitude\",}\n",
    "nbi.hist(get_data_for_hist_errors, n=(1,2000), eps=(0.01,1, 0.01), delta=(0.01, 1, 0.01), distribution={'normal': \"normal\", 'zipf': 'zipf', 'uniform': 'uniform', 'exp':'exp', 'geometric':'geometric', 'lognorm':'lognorm'}, options=opts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does data spread affect our errors?\n",
    "Let’s take this a step further and scope out the exact effect on standard deviation (a mathematical proxy for the “spread” of the data) on the errors. Let’s investigate what happens when we sample n = 1000 integers from a normal distribution with a mean of μ = 0. We vary the standard deviation between 1 and 100 to understand how spreading the distribution affects errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     7,
     26
    ]
   },
   "outputs": [],
   "source": [
    "# COUNT MIN ACROSS SDS\n",
    "def get_sample_sd(eps, delta, n, sd): \n",
    "    return generate_sample(n=n, dist=\"normal\", scale=sd)\n",
    "def get_y_errors_sd(xs, n, eps, delta, sd):\n",
    "    n = n\n",
    "    threshold = eps * n\n",
    "\n",
    "    mean = 0\n",
    "    sd = 100\n",
    "    sample = xs\n",
    "\n",
    "    cms = CountMinSketch(eps, delta)\n",
    "    load_data(cms, sample)\n",
    "\n",
    "    p, avg_err, max_err, err = compute_error_prob(cms, sample, n)\n",
    "\n",
    "    print(\"Average Error: \" + str(avg_err))\n",
    "    print(\"Maximum Error: \" + str(max_err))\n",
    "    print(\"Acceptable Threshold: \" + str(threshold))\n",
    "    print(\"Proportion of Errors Exceeding Threshold: \" + str(p))\n",
    "\n",
    "    return err\n",
    "def get_data_for_hist_errors_sd(n, eps, delta, sd):\n",
    "    n = n\n",
    "    threshold = eps * n\n",
    "\n",
    "    mean = 0\n",
    "    sd = 100\n",
    "    sample = get_sample_sd(eps, delta, n, sd)\n",
    "    cms = CountMinSketch(eps, delta)\n",
    "    load_data(cms, sample)\n",
    "\n",
    "    p, avg_err, max_err, err = compute_error_prob(cms, sample, n)\n",
    "\n",
    "    print(\"Average Error: \" + str(avg_err))\n",
    "    print(\"Maximum Error: \" + str(max_err))\n",
    "    print(\"Acceptable Threshold: \" + str(threshold))\n",
    "    print(\"Proportion of Errors Exceeding Threshold: \" + str(p))\n",
    "\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'title': \"Scatterplot of Errors\",\n",
    "    'ylabel': \"Error\",\n",
    "    'xlabel': \"Data Point\",\n",
    "    'ylim': (0, 500),}\n",
    "print(\"Vary the standard distribution to see how the errors change!\")\n",
    "\n",
    "nbi.scatter(get_sample_sd, get_y_errors_sd, n=(1,2000), eps=(0.01,1, 0.01), delta=(0.01, 1, 0.01), sd=(0.01, 1000, 10), options=opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'title': \"Distribution of errors\",\n",
    "    'ylabel': \"Count\",\n",
    "    'xlabel': \"Error Magnitude\",}\n",
    "print(\"Vary the standard distribution to see how error disribution change!\")\n",
    "nbi.hist(get_data_for_hist_errors_sd, n=(1,2000), eps=(0.01,1, 0.01), delta=(0.01, 1, 0.01),sd=(0.01, 1000, 10),  options=opts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization #1: The Learned Count-Min Sketch\n",
    "\n",
    "One approach is simply to treat the heavy hitters and non-heavy-hitters separately. This is where we can motivate our data structure design with two ideas from the original Learned Index Structures paper –– recursive models and auxiliary structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LearnedCountMinSketch:\n",
    "    def __init__(self, eps, delta, train_data):\n",
    "        self.eps = eps\n",
    "        self.delta = delta\n",
    "        self.cms = CountMinSketch(eps, delta)\n",
    "\n",
    "        # set model\n",
    "        X_train = train_data[0]\n",
    "        Y_train = train_data[1]\n",
    "        self.model = MLPClassifier(hidden_layer_sizes=(30, 40))\n",
    "        self.model.fit(X_train.reshape(-1, 1), np.ravel(Y_train))\n",
    "        self.perfect = {}\n",
    "\n",
    "    def count(self, value):\n",
    "        if (self.model.predict(np.array([value]).reshape(-1, 1)) == 1):\n",
    "            if str(value) in self.perfect:\n",
    "                self.perfect[str(value)] = self.perfect[str(value)] + 1\n",
    "            else:\n",
    "                self.perfect[str(value)] = 1\n",
    "        else:\n",
    "            self.cms.count(value)\n",
    "\n",
    "    def estimate(self, value):\n",
    "        if (self.model.predict(np.array([value]).reshape(-1, 1)) == 1):\n",
    "            if str(value) in self.perfect: return self.perfect[str(value)]\n",
    "            return 0\n",
    "        else:\n",
    "            return self.cms.estimate(value)\n",
    "\n",
    "    def real_estimate(self, value):\n",
    "        if str(value) in self.perfect: return self.perfect[str(value)]\n",
    "        if str(value) in self.cms.backup: return self.cms.backup[str(value)]\n",
    "        return -1\n",
    "\n",
    "    def compute_size(self):\n",
    "        size = 0\n",
    "        for key in self.cms.backup:\n",
    "            size += abs(self.cms.backup[key])\n",
    "        for key in self.perfect:\n",
    "            size += abs(self.perfect[key])\n",
    "        return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# label data --> threshold is what proportion of data points you want to call \"heavy hitters\"\n",
    "def label_sample(sample, p = 0.05):\n",
    "    n = len(sample)\n",
    "    n_distinct = len(Counter(sample).keys())\n",
    "    num = int(n_distinct * p)\n",
    "    X_train = np.array(sample)\n",
    "    Y_train = np.zeros_like(X_train)\n",
    "    hh = Counter(sample).most_common(num)\n",
    "    hh = set([el[0] for el in hh])\n",
    "    for i in range(n):\n",
    "        if X_train[i] in hh:\n",
    "            Y_train[i] = 1\n",
    "    return X_train, Y_train, hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     164,
     171,
     179,
     183,
     186,
     197
    ]
   },
   "outputs": [],
   "source": [
    "# VISUALIZE ML LEARNED COUNT MIN SKETCH\n",
    "\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import json \n",
    "\n",
    "class CountMinSketch:\n",
    "    def __init__(self, eps, delta):\n",
    "        self.eps = eps\n",
    "        self.delta = delta\n",
    "        self.w = math.ceil(np.exp(1) / eps)\n",
    "        self.d = math.ceil(np.log(1 / delta))\n",
    "        self.tables = np.zeros((self.d, self.w))\n",
    "        self.backup = {}\n",
    "\n",
    "    def compute_hash(self, value, table_no):\n",
    "        fn = hashlib.md5()\n",
    "        inp = str(value) + str(0) + str(table_no)\n",
    "        fn.update(inp.encode())\n",
    "        out = int(fn.hexdigest(), 16)\n",
    "        return out % self.w\n",
    "\n",
    "    def count(self, value):\n",
    "        if str(value) in self.backup: \n",
    "            self.backup[str(value)] = self.backup[str(value)] + 1\n",
    "        else:\n",
    "            self.backup[str(value)] = 1\n",
    "        for i in range(self.d):\n",
    "            j = self.compute_hash(value, i)\n",
    "            self.tables[i][j] = self.tables[i][j] + 1\n",
    "\n",
    "    def estimate(self, value):\n",
    "        ests = []\n",
    "        for i in range(self.d):\n",
    "            j = self.compute_hash(value, i)\n",
    "            ests.append(self.tables[i][j])\n",
    "        return min(ests)\n",
    "\n",
    "    def real_estimate(self, value):\n",
    "        if str(value) in self.backup: return self.backup[str(value)]\n",
    "        return -1\n",
    "\n",
    "    def compute_size(self):\n",
    "        size = 0\n",
    "        for key in self.backup:\n",
    "            size += abs(self.backup[key])\n",
    "        return size\n",
    "\n",
    "    def save_counts(self, count_filename='counts.txt', actual_filename='backups.txt'):\n",
    "        np.savetxt(count_filename, self.tables)\n",
    "        with open(actual_filename, 'w') as fp: json.dump(self.backup, fp)\n",
    "\n",
    "    def load_counts(self, count_filename='counts.txt', actual_filename='backups.txt'):\n",
    "        with open(actual_filename, 'r') as fp: \n",
    "            temp = json.load(fp)\n",
    "            self.backup = temp\n",
    "        self.tables = np.loadtxt(count_filename)\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "def generate_sample(n=1000, dist='uniform', loc=0, scale=1000, lambda_=5, s=1):\n",
    "  if dist == 'uniform':\n",
    "    float_sample = stats.uniform.rvs(loc, scale, n)\n",
    "    return [int(el) for el in float_sample]\n",
    "  if dist == 'zipf':\n",
    "    float_sample = stats.zipf.rvs(loc + 1, size=n)\n",
    "    return [int(el) for el in float_sample]\n",
    "  if dist == 'exp':\n",
    "    float_sample = stats.planck.rvs(lambda_, size=n)\n",
    "    return [int(el) for el in float_sample]\n",
    "  if dist == 'lognorm':\n",
    "    float_sample = stats.lognorm.rvs(s=scale, size=n)\n",
    "    return [int(el) for el in float_sample]\n",
    "  if dist == 'geometric':\n",
    "    float_sample =  stats.geom.rvs(p, size=n)\n",
    "    return [int(el) for el in float_sample]\n",
    "  elif dist == 'normal':\n",
    "    float_sample = stats.norm.rvs(loc, scale, n)\n",
    "    return [int(el) for el in float_sample]\n",
    "  else:\n",
    "    return -1\n",
    "\n",
    "for el in uniform_sample:\n",
    "  cms.count(el)\n",
    "\n",
    "def compute_error_prob(cms, data, n):\n",
    "  err = []\n",
    "  for el in data:\n",
    "    err.append(cms.estimate(el) - cms.real_estimate(el))\n",
    "  avg_err = sum(err) / len(err)\n",
    "  max_err = max(err)\n",
    "  exceed = 0\n",
    "  for el in err:\n",
    "    if el > cms.eps * n:\n",
    "      exceed += 1\n",
    "  return exceed / len(err), err\n",
    "\n",
    "def load_data(cms, data):\n",
    "  for el in data:\n",
    "    cms.count(el)\n",
    "\n",
    "def run_experiment():\n",
    "  n = 1000\n",
    "  eps = 0.01\n",
    "  min_delta = 0.01\n",
    "  max_delta = 0.1\n",
    "  deltas = np.linspace(min_delta, max_delta, 10).tolist()\n",
    "  # deltas = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "  ps = []\n",
    "  for delta in deltas:\n",
    "    probs = 0\n",
    "    for i in range(3):\n",
    "      cms = CountMinSketch(eps, delta)\n",
    "      print(cms.w)\n",
    "      dt = generate_sample(n)\n",
    "      load_data(cms, dt)\n",
    "      p, err = compute_error_prob(cms, dt, n)\n",
    "      print(err)\n",
    "      probs += p\n",
    "    probs /= 3\n",
    "    ps.append(probs)\n",
    "  return deltas, ps, err\n",
    "\n",
    "# VISUALIZE ERROR PROBABILITY VS DELTA\n",
    "\n",
    "# compute empirical probability of error exceeding the threshold\n",
    "def compute_error_prob_ml(cms, data, n):\n",
    "  err = []\n",
    "  for el in data:\n",
    "    err.append(cms.estimate(el) - cms.real_estimate(el))\n",
    "  avg_err = sum(err) / len(err)\n",
    "  max_err = max(err)\n",
    "  exceed = 0\n",
    "  for el in err:\n",
    "    if el > cms.eps * n:\n",
    "      exceed += 1\n",
    "  p = exceed / len(err)\n",
    "  return p, avg_err, max_err, err\n",
    "\n",
    "# run experiments on 10 values of delta interpolated between (min_delta, max_delta) and compute array of corresponding error probabilities\n",
    "def error_prob_vs_delta(n=100000, eps=0.4, min_delta=0.01, max_delta=0.1):\n",
    "  deltas = np.linspace(min_delta, max_delta, 10).tolist()\n",
    "  # deltas = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "  ps = []\n",
    "  for delta in deltas:\n",
    "    probs = 0\n",
    "    # Average probabilities across 3 trials\n",
    "    for i in range(3):\n",
    "      cms = CountMinSketch(eps, delta)\n",
    "      dt = generate_sample(n)\n",
    "      load_data(cms, dt)\n",
    "      p, avg_err, max_err, err = compute_error_prob(cms, dt, n)\n",
    "      probs += p\n",
    "    probs /= 3\n",
    "    ps.append(probs)\n",
    "  return deltas, ps\n",
    "\n",
    "# graphing helper function\n",
    "def graph_error_prob_vs_delta(deltas, ps, filename=\"p_vs_delta.png\"):\n",
    "  plt.scatter(deltas, ps)\n",
    "  plt.show()\n",
    "\n",
    "    \n",
    "# compute empirical probability of error exceeding the threshold\n",
    "def compute_error_prob(cms, data, n):\n",
    "  err = []\n",
    "  for el in data:\n",
    "    err.append(cms.estimate(el) - cms.real_estimate(el))\n",
    "  avg_err = sum(err) / len(err)\n",
    "  max_err = max(err)\n",
    "  exceed = 0\n",
    "  for el in err:\n",
    "    if el > cms.eps * n:\n",
    "      exceed += 1\n",
    "  p = exceed / len(err)\n",
    "  return p, avg_err, max_err, err\n",
    "  #return err\n",
    "\n",
    "# run experiments on 10 values of delta interpolated between (min_delta, max_delta) and compute array of corresponding error probabilities\n",
    "def error_prob_vs_delta(n=100000, eps=0.4, min_delta=0.01, max_delta=0.1):\n",
    "  deltas = np.linspace(min_delta, max_delta, 10).tolist()\n",
    "  # deltas = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "  ps = []\n",
    "  for delta in deltas:\n",
    "    probs = 0\n",
    "    # Average probabilities across 3 trials\n",
    "    for i in range(3):\n",
    "      cms = CountMinSketch(eps, delta)\n",
    "      dt = generate_sample(n)\n",
    "      load_data(cms, dt)\n",
    "      p, avg_err, max_err, err = compute_error_prob(cms, dt, n)\n",
    "      probs += p\n",
    "    probs /= 3\n",
    "    ps.append(probs)\n",
    "  return deltas, ps\n",
    "\n",
    "# graphing helper function\n",
    "def graph_error_prob_vs_delta(deltas, ps, filename=\"p_vs_delta.png\"):\n",
    "  plt.scatter(deltas, ps)\n",
    "  plt.show()\n",
    "    \n",
    "def opt_1_normal(sd):\n",
    "    eps = 0.01\n",
    "    delta = 0.05\n",
    "    n = 1000\n",
    "    threshold = eps * n\n",
    "    mean = 0\n",
    "    p = 0.2\n",
    "    sample = generate_sample(n=n, dist='normal', loc=mean, scale=sd)\n",
    "    X_tr, Y_tr, hh = label_sample(sample, p)\n",
    "\n",
    "    cms = CountMinSketch(eps, delta)\n",
    "\n",
    "    load_data(cms, sample)\n",
    "    p, avg_err, max_err, err = compute_error_prob(cms, sample, n)\n",
    "    return err\n",
    "\n",
    "def opt_1_learned(sd):\n",
    "    eps = 0.01\n",
    "    delta = 0.05\n",
    "    n = 1000\n",
    "    threshold = eps * n\n",
    "    mean = 0\n",
    "    \n",
    "    p = 0.2\n",
    "    sample = generate_sample(n=n, dist='normal', loc=mean, scale=sd)\n",
    "    X_tr, Y_tr, hh = label_sample(sample, p)\n",
    "\n",
    "    lcms = LearnedCountMinSketch(eps, delta, [X_tr, Y_tr])\n",
    "\n",
    "    load_data(lcms, sample)\n",
    "    p, avg_err, max_err, err = compute_error_prob(lcms, sample, n)\n",
    "    return err\n",
    "\n",
    "class RuleCountMinSketch:\n",
    "    def __init__(self, eps, delta, hh):\n",
    "        self.eps = eps\n",
    "        self.delta = delta\n",
    "        self.cms = CountMinSketch(eps, delta)\n",
    "        self.hh = hh\n",
    "        self.perfect = {}\n",
    "\n",
    "    def count(self, value):\n",
    "        if value in self.hh:\n",
    "            if str(value) in self.perfect:\n",
    "                self.perfect[str(value)] = self.perfect[str(value)] + 1\n",
    "            else:\n",
    "                self.perfect[str(value)] = 1\n",
    "        else:\n",
    "            self.cms.count(value)\n",
    "\n",
    "    def estimate(self, value):\n",
    "        if (value in self.hh):\n",
    "            if str(value) in self.perfect: return self.perfect[str(value)]\n",
    "            return 0\n",
    "        else:\n",
    "            return self.cms.estimate(value)\n",
    "\n",
    "    def real_estimate(self, value):\n",
    "        if str(value) in self.perfect: return self.perfect[str(value)]\n",
    "        if str(value) in self.cms.backup: return self.cms.backup[str(value)]\n",
    "        return -1\n",
    "\n",
    "    def compute_size(self):\n",
    "        size = 0\n",
    "        for key in self.cms.backup:\n",
    "            size += abs(self.cms.backup[key])\n",
    "        for key in self.perfect:\n",
    "            size += abs(self.perfect[key])\n",
    "        return size\n",
    "    \n",
    "def opt_2(sd):\n",
    "    eps = 0.01\n",
    "    delta = 0.05\n",
    "    n = 1000\n",
    "    threshold = eps * n\n",
    "    mean = 0\n",
    "    p = 0.2\n",
    "    sample = generate_sample(n=n, dist='normal', loc=mean, scale=sd)\n",
    "    X_tr, Y_tr, hh = label_sample(sample, p)\n",
    "\n",
    "    rcms = RuleCountMinSketch(eps, delta, hh)\n",
    "\n",
    "    load_data(rcms, sample)\n",
    "    p, avg_err, max_err, err = compute_error_prob(rcms, sample, n)\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'title': \"Count-Min Sketch\",\n",
    "    'ylabel': \"Frequency\",\n",
    "    'xlabel': \"Error\",}\n",
    "print(\"Vary the standard distribution to see how error disribution change!\")\n",
    "nbi.hist(opt_1_normal, sd=(0, 1000, 10),  options=opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'title': \"Learned Count-Min Sketch\",\n",
    "    'ylabel': \"Frequency\",\n",
    "    'xlabel': \"Error\",}\n",
    "print(\"Vary the standard distribution to see how error disribution change!\")\n",
    "nbi.hist(opt_1_normal, sd=(0, 1000, 10),  options=opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'title': \"Rule Based Count-Min Sketch\",\n",
    "    'ylabel': \"Frequency\",\n",
    "    'xlabel': \"Error\",}\n",
    "print(\"Vary the standard distribution to see how error disribution change!\")\n",
    "nbi.hist(opt_2, sd=(0, 1000, 10),  options=opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbi.publish('markiewagner/cs166-proj-final/master', 'demo.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
